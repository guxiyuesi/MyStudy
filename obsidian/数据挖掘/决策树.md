## 决策树

### 基本思想
![](https://gitee.com/four_four/picgo/raw/master/img/20211214201247.png)

### 决策树的基本组成
+ 测试节点<br>
表示某种作为判断条件的属性
+ 分支<br>根据属性条件取值选择的路径
+ 叶子<br>使判断终止的结论

![](https://gitee.com/four_four/picgo/raw/master/img/20211214201504.png)

### Hunt算法
#### 算法步骤
![](https://gitee.com/four_four/picgo/raw/master/img/20211214201639.png)

#### 特点
贪心策略, 局部最优

#### 设计问题
+ 如何分裂训练记录
	+ [[决策树#怎样为不同类型的属性指定测试条件|怎样为不同类型的属性指定测试条件]]
	+ [[决策树#纯度的测量|怎样评估每种测试条件]]
+ 如何停止分裂过程


#### 怎样为不同类型的属性指定测试条件
依赖于属性的类型:
+ 标称
+ 序数
+ 连续
依赖于划分的路数:
+ 多路划分(多叉树)
+ 二路划分(二叉树)

##### 基于标称属性的划分
+ 多路划分
+ 二路划分

![](https://gitee.com/four_four/picgo/raw/master/img/20211214202332.png)

##### 基于连续属性的划分
![](https://gitee.com/four_four/picgo/raw/master/img/20211214202355.png)


#### 怎样选择最佳划分
依据纯性
![](https://gitee.com/four_four/picgo/raw/master/img/20211214202457.png)

##### 计算纯性
用信息熵代表纯性
![](https://gitee.com/four_four/picgo/raw/master/img/20211214202833.png)

熵值越低, 数据越纯

### 信息增益算法(ID3)
![](https://gitee.com/four_four/picgo/raw/master/img/20211214203314.png)
#### 信息增益算法的优点
1.  决策树易于理解和解释，可以可视化分析。
2.  决策树分类器的构造不需要任何领域知识或参数设置，
3.  适合高维数据
4.  可以同时处理标称型和数值型数据
5.  计算复杂度不高

#### 信息增益算法的缺点
1.  容易出现过拟合
2.  对缺失数据处理比较困难
3.  忽略数据集中属性的相关性
4.  ID3算法计算信息增益时偏向数值较多的特征
5.  不支持增量学习


#### 纯度的测量
1. [[决策树#计算纯性|信息熵]]
2. [[决策树#GINI指数|GINI指数]]
3. [[决策树#最大分类错误数|最大分类错误数]]

##### GINI指数
![](https://gitee.com/four_four/picgo/raw/master/img/20211214210817.png)

![](https://gitee.com/four_four/picgo/raw/master/img/20211214210832.png)

##### 最大分类错误数
![](https://gitee.com/four_four/picgo/raw/master/img/20211214210954.png)

![](https://gitee.com/four_four/picgo/raw/master/img/20211214211003.png)

### 连续数据问题
![](https://gitee.com/four_four/picgo/raw/master/img/20211214211151.png)

连续数据离散化, 选择最佳划分点

#### 二元划分 选择最佳划分点
从最小值开始建立分割区间, 计算各自的信息增益, 选择 **信息增益最大** 的一个分割区间作为最佳划分点

### 剪枝
![](https://gitee.com/four_four/picgo/raw/master/img/20211214211636.png)

过拟合: 训练集少, 模型复杂
欠拟合: 训练集多, 模型简单

### 决策树总结
![](https://gitee.com/four_four/picgo/raw/master/img/20211214211843.png)
